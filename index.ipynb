{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this exercise we will attempt to classify text messages as \"SPAM\" or \"HAM\" using TF-IDF Vectorization. Once we successfully classify our texts we will examine our results to see which words are most important to each class of text messages. \n",
    "\n",
    "Complete the functions below and answer the question(s) at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2  target\n",
       "0   ham  Go until jurong point, crazy.. Available only ...       0\n",
       "1   ham                      Ok lar... Joking wif u oni...       0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3   ham  U dun say so early hor... U c already then say...       0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "df_messages = pd.read_csv('data/spam.csv', usecols=[0,1])\n",
    "\n",
    "# convert string labels to 1 or 0 \n",
    "le = LabelEncoder()\n",
    "df_messages['target'] = le.fit_transform(df_messages['v1'])\n",
    "\n",
    "# examine our data\n",
    "df_messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# separate features and labels \n",
    "X = df_messages['v2']\n",
    "y = df_messages['target']\n",
    "\n",
    "# generate a list of stopwords for TfidfVectorizer to ignore\n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "#stopwords_list[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>1) Let's create a function that takes in our various texts along with their respective labels and uses TF-IDF to vectorize the texts.  Recall that TF-IDF helps us \"vectorize\" text (turn text into numbers) so we can do \"math\" with it.  It is used to reflect how relevant a term is in a given document in a numerical way. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate tf-idf vectorization (use sklearn's TfidfVectorizer) for our data\n",
    "import sklearn\n",
    "\n",
    "def tfidf(X, y,  stopwords_list): \n",
    "    '''\n",
    "    Generate train and test TF-IDF vectorization for our data set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pandas.Series object\n",
    "        Pandas series of text documents to classify \n",
    "    y : pandas.Series object\n",
    "        Pandas series containing label for each document\n",
    "    stopwords_list: list ojbect\n",
    "        List containing words and punctuation to remove. \n",
    "    Returns\n",
    "    --------\n",
    "    tf_idf_train :  sparse matrix, [n_train_samples, n_features]\n",
    "        Vector representation of train data\n",
    "    tf_idf_test :  sparse matrix, [n_test_samples, n_features]\n",
    "        Vector representation of test data\n",
    "    y_train : array-like object\n",
    "        labels for training data\n",
    "    y_test : array-like object\n",
    "        labels for testing data\n",
    "    vectorizer : vectorizer object\n",
    "        fit TF-IDF vecotrizer object\n",
    "\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7398 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 55050 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_train, tf_idf_test, y_train, y_test, vectorizer = tfidf(X, y, stopwords_list)\n",
    "\n",
    "tf_idf_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>2) Now that we have a set of vectorized training data we can use this data to train a classifier to learn how to classify a specific text based on the vectorized version of the text. Below we have initialized a simple Naive Bayes Classifier and Random Forest Classifier. Complete the function below which will accept a classifier object, a vectorized training set, vectorized test set, and list of training labels and return a list of predictions for our training set and a separate list of predictions for our test set.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a function that takes in a classifier, trains it on our tf-idf vectors,\n",
    "# and generates train and test predictiions\n",
    "def classify_text(classifier, tf_idf_train, tf_idf_test, y_train):\n",
    "    '''\n",
    "    Train a classifier to identify whether a message is spam or ham\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    classifier: sklearn classifier\n",
    "       initialized sklearn classifier (MultinomialNB, RandomForestClassifier, etc.)\n",
    "    tf_idf_train : sparse matrix, [n_train_samples, n_features]\n",
    "        TF-IDF vectorization of train data\n",
    "    tf_idf_test : sparse matrix, [n_test_samples, n_features]\n",
    "        TF-IDF vectorization of test data\n",
    "    y_train : pandas.Series object\n",
    "        Pandas series containing label for each document in the train set\n",
    "    Returns\n",
    "    --------\n",
    "    train_preds :  list object\n",
    "        Predictions for train data\n",
    "    test_preds :  list object\n",
    "        Predictions for test data\n",
    "    '''\n",
    "    # your code here\n",
    "    # a) fit the classifier with our training data\n",
    "    classifier.fit(tf_idf_train, y_train)\n",
    "    \n",
    "    # b) predict the labels of our train data and store them in train_preds\n",
    "    train_preds = classifier.predict(tf_idf_train)\n",
    "    \n",
    "    # c) predict the labels of our test data and store them in test_preds\n",
    "    test_preds = classifier.predict(tf_idf_test)    \n",
    "    \n",
    "    # d) return train_preds and test_preds\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1202    0]\n",
      " [  56  135]]\n",
      "0.9597989949748744\n"
     ]
    }
   ],
   "source": [
    "# generate predictions with Naive Bayes Classifier\n",
    "nb_train_preds, nb_test_preds = classify_text(nb_classifier, tf_idf_train, tf_idf_test, y_train)\n",
    "\n",
    "# evaluate performance of Naive Bayes Classifier\n",
    "print(confusion_matrix(y_test, nb_test_preds))\n",
    "print(accuracy_score(y_test, nb_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1202    0]\n",
      " [  34  157]]\n",
      "0.9755922469490309\n"
     ]
    }
   ],
   "source": [
    "# generate predictions with Random Forest Classifier\n",
    "rf_train_preds, rf_test_preds = classify_text(rf_classifier, tf_idf_train, tf_idf_test, y_train)\n",
    "\n",
    "# evaluate performance of Random Forest Classifier\n",
    "print(confusion_matrix(y_test, rf_test_preds))\n",
    "print(accuracy_score(y_test, rf_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can see both classifiers do a pretty good job classifying texts as either \"SPAM\" or \"HAM\". Let's figure out which words are the most important to each class of texts! Recall that Inverse Document Frequency can help us determine which words are most important in an entire corpus or group of documents. \n",
    "\n",
    "<b>3) Create a function that calculates the inverse document frequency (IDF) of each word in our collection of texts.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_idf(class_, df, stopwords_list):\n",
    "    '''\n",
    "    Get ten words with lowest IDF values representing 10 most important\n",
    "    words for a defined class (spam or ham)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    class_ : str object\n",
    "        string defining class 'spam' or 'ham'\n",
    "    df : pandas DataFrame object\n",
    "        data frame containing texts and labels\n",
    "    stopwords_list: list object\n",
    "        List containing words and punctuation to remove. \n",
    "    --------\n",
    "    important_10 : pandas dataframe object\n",
    "        Dataframe containing 10 words and respective IDF values\n",
    "        representing the 10 most important words found in the texts\n",
    "        associated with the defined class\n",
    "    '''\n",
    "    # your code here\n",
    "       \n",
    "    # a) generate series containing all texts associated with the defined class\n",
    "   # b) initialize dictionary to count document frequency \n",
    "    # (number of documents that contain a certain word)\n",
    "    # c) loop over each text and split each text into a list of its unique words \n",
    "        # d) loop over each word and if it is not in the stopwords_list add the word \n",
    "        #    to class_dict with a value of 1. if it is already in the dictionary\n",
    "        #    increment it by 1                  \n",
    "    # e) take our dictionary and calculate the \n",
    "    #    IDF (number of docs / number of docs containing each word) \n",
    "    #    for each word\n",
    "    # f) return the 10 words with the lowest IDF \n",
    "\n",
    "    import math\n",
    "    words = {}\n",
    "    class_dict = {}\n",
    "    docs = df_messages[df_messages['v1'] == 'spam'].v2\n",
    "  \n",
    "    for doc in docs:\n",
    "        words = set(doc.split()).union(words_b)\n",
    "\n",
    "    for word in words:\n",
    "        if not word in stopwords_list:\n",
    "            class_dict[word]=+1\n",
    "    print(class_dict)\n",
    "    N = len(docs)\n",
    "    print(N)\n",
    "    for word, val in class_dict.items():\n",
    "        class_dict[word] = math.log(N / val)\n",
    "\n",
    "\n",
    "        \n",
    "    return list(class_dict.items())[0:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 1, 'This': 1, 'tried': 1, 'Pound': 1, 'per': 1, 'contact': 1, '10p': 1, 'u.': 1, '�750': 1, 'claim': 1, '2': 1, '087187272008': 1, 'easy,': 1, 'call': 1, 'NOW1!': 1, 'Only': 1, 'U': 1, 'prize.': 1, '2nd': 1, 'minute.': 1, 'BT-national-rate.': 1}\n",
      "747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('time', 6.616065185132817),\n",
       " ('This', 6.616065185132817),\n",
       " ('tried', 6.616065185132817),\n",
       " ('Pound', 6.616065185132817),\n",
       " ('per', 6.616065185132817),\n",
       " ('contact', 6.616065185132817),\n",
       " ('10p', 6.616065185132817),\n",
       " ('u.', 6.616065185132817),\n",
       " ('�750', 6.616065185132817),\n",
       " ('claim', 6.616065185132817),\n",
       " ('2', 6.616065185132817)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_idf('spam', df_messages, stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.616065185132817,\n",
       " 6.616065185132817,\n",
       " 6.616065185132817,\n",
       " 6.616065185132817,\n",
       " 6.616065185132817,\n",
       " 6.616065185132817,\n",
       " 6.616065185132817,\n",
       " 6.616065185132817,\n",
       " 6.616065185132817,\n",
       " 6.616065185132817,\n",
       " 6.616065185132817]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_idf('ham', df_messages, stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Explain\n",
    "<b> 4) Imagine that the word \"school\" has the highest TF-IDF value in the second document of our test data. What does that tell us about the word school? </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# This means that school is the rarest token in the second document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
